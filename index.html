<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0024)https://siyuanhuang.com/ -->
<html class="gr__siyuanhuang_com"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154737113-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154737113-1');
</script>

<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
<style type="text/css">
a {
color: #000080;
text-decoration:none;
}

a:focus, a:hover {
color: #000080;
text-decoration:underline;
}

a.hover
{
    text-decoration: none;
}

 a.hover:hover
 {
    text-decoration: underline;
 }
body,td,th {
	font-family: Lora;
	font-size: 16px;
}
papertitle {
	font-family: Lora;
	font-size: 16px;
}
conference {
	font-family: Lora;
	font-size: 16px;
	color: #696969;
}
strong {
	font-family: Lora;
	font-size: 16px;
	font-weight: 600;
}
heading {
	font-family: Lora;
	color: #970024; /*#EB6400;*/
	font-size: 28px;
	font-weight: bold;
}
pvenue {
	display: inline-block;
	color: #500;
}
pkeywords{
	color: #EB6400;
}
</style>
    <!-- Custom Fonts -->
    <link href="./resources/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="./resources/css" rel="stylesheet" type="text/css">
    <link href="./resources/css(1)" rel="stylesheet" type="text/css">
<link rel="icon" type="image/png" href="https://siyuanhuang.com/seal_icon.png">
<link href="./resources/styles.css" rel="stylesheet">
<title>Xu Xie - VCLA, UCLA</title>

<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<link href="./resources/css(2)" rel="stylesheet" type="text/css">
</head>
<body data-gr-c-s-loaded="true">
<div id="Banner">
    <div height="80" id="header" style="background-color:#000080; color: #000080">
      <center>
        <table width="1050" height="80" border="0">
          <tbody><tr>
		<td halign="center">
			<p align="center"><font size="6"><font color="#FFFFFF">Xu Xie</font> </font></p><font size="6">
		</font></td>
	</tr>
        </tbody></table>
      </center>
    </div>
	<table width="1000" border="0" align="center" cellpadding="10">

	<tbody><tr>
		<td>
			<table width="100%" border="0" align="center" cellpadding="10">
			<tbody><tr>
							
				<td width="15%" valign="top">
					<img src="./resources/self.png" alt="Seattle" width="100%">
				</td>
				<td width="50%" valign="top">
			I am a Fourth year Ph.D. student in the Department of Statistics at University of California, Los Angeles. I am advised by <a href="http://www.stat.ucla.edu/~sczhu/">Professor Song-Chun Zhu</a> at the <a href="https://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>. Before coming to UCLA, I graduated from <a href="https://www.ustc.edu.cn/">University of Science and Technology of China</a> with a B.S. of Electronic Information Engineering in <a href="https://en.scgy.ustc.edu.cn/">School for the Gifted Young</a>.<p></p>
			<br>My research interests lies on robotics, virtual reality, (inverse) reinforcement learning. I currently focus on the problem of modeling agent intention in the driving scenario which involves agent mind, task plan and value estimation. My research highlights achieving machine intelligence through simulations. I am leveraging simulation to promote robot learning on complicated tasks. 
         
          <center>
          <a href="mailto:xiexu@ucla.edu">E-Mail</a> / 
          <a href="./resources/XieXu-CV.pdf">CV</a> 
          <!-- <a href="https://scholar.google.com/citations?user=1NN7Ee8AAAAJ&amp;hl=en&amp;citsig=AMstHGQ_eDX956QVraGalRBAEWGG0ltzRw"> Google Scholar</a> -->
          </center>

			</td></tr>
			</tbody></table>
		</td>
	</tr>

	<tr>
		<td>
      <heading>News</heading>
      <ul>
      	<li> <font color="red"><strong>NEW</strong></font>&nbsp;&nbsp; 02/2020 Research intern offer @ Facebook Reality Labs, summer 2020. </li>
      	<li> 12/2019 One Paper published in Science Robotics 2019. </li>
		<li> 06/2019 One Paper accepted by IROS 2019.</li>
		<li> Best Paper Award in <a href="https://www.acmturc.com/2019">ACM Turing Celebration Conference (ACM TURC) 2019 </a>.</li>
      	<li> 03/2019 One Paper accepted by ACM TURC 2019.
      	</li><li> 05/2019 One Paper accepted by ICML Workshop 2019.
      	</li><li> 01/2019 One Paper accepted by ICRA 2019.
      	</li><li> 01/2019 One Paper accepted by AAAI Workshop 2019.
      	</li><li> 01/2018 Two Papers accepted by ICRA 2018. </li>
		<li> 06/2017 Two Papers accepted by IROS 2017. </li>
	</ul> <br><br><br>
     
   
      <heading>Publications</heading>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/scirot19.png" alt="SciRot 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>A Tale of Two Explanations: Enhancing Human Trust by Explaining Robot Behavior</papertitle> <br>
					<a href="http://www.mjedmonds.com/" ,="" style="color : #696969;">Mark Edmonds</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<strong>Xu Xie</strong>,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock/" ,="" style="color : #696969;">Brandon Rothrock</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="http://www.stat.ucla.edu/~ywu/" ,="" style="color : #696969;">Yingnian Wu</a>,
					<a href="https://www.psych.ucla.edu/faculty/page/hongjing" ,="" style="color : #696969;">Hongjing Lu</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					Science Robotics 2019 <br>
 					<a href="https://robotics.sciencemag.org/content/4/37/eaay4663">Paper</a> /
					<a href="https://robotics.sciencemag.org/content/suppl/2019/12/16/4.37.eaay4663.DC1">Video</a> /
					<a href="https://robotics.sciencemag.org/content/suppl/2019/12/16/4.37.eaay4663.DC1">Code</a> /
					<a href="https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-robot-that-explains-its-actions">IEEE Spectrum</a> /
					<a href="https://samueli.ucla.edu/in-robots-we-trust-ucla-study-shows-way-forward/">UCLA Samueli School of Engineering</a> <br>
					<pkeywords> The ability to provide comprehensive explanations of chosen actions is a hallmark of intelligence. Lack of this ability impedes the general acceptance of AI and robot systems in critical tasks. This paper examines what forms of explanations best foster human trust in machines and proposes a framework in which explanations are generated from both functional and mechanistic perspectives.  </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/iros19.png" alt="IROS 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Learning Virtual Grasp with Failed Demonstrations via Bayesian Inverse Reinforcement Learning</papertitle> <br>
					<strong>Xu Xie</strong>,
					<a href="https://changyangli.github.io/" ,="" style="color : #696969;">Changyang Li</a>,
					<a href="http://wellyzhang.github.io/" ,="" style="color : #696969;">Chi Zhang</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Intelligent Robots and Systems (IROS) 2019 <br>
 					<a href="./resources/iros19xie.pdf">Paper</a> /
					<a href="https://vimeo.com/350872475">Demo</a> /
					<a href="https://xuxie1031.github.io/projects/VRGrasp/VRGraspProj.html">Project</a> /
					<a href="https://github.com/xuxie1031/VRGraspIRLEnv">Code</a> <br>
					<pkeywords> Propose Bayesian Inverse Reinforcement Learning with Failure (BIRLF), which makes use of failed demonstrations that were often ignored or filtered in previous methods due to the difficulties to incorporate them in addition to the successful ones. </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/acmturc19.gif" alt="ACMTURC 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>VRGym: A Virtual Testbed for Physical and Interactive AI</papertitle> <br>
					<strong>Xu Xie</strong>,
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://www.zlz.link/" ,="" style="color : #696969;">Zhenliang Zhang</a>,
					<a href="https://yuxinqiu.github.io/" ,="" style="color : #696969;">Yuxing Qiu</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					ACM Turing Celebration Conference (ACM TURC) 2019 <br>
 					<a href="./resources/sigai19xie.pdf">Paper</a> /
					<a href="./resources/sigai19xie_sppl.pdf">Supplementary</a> /
					<a href="https://vimeo.com/327629749">Demo</a> /
					<a href="https://xuxie1031.github.io/projects/VRGym/VRGymProj.html">Project</a> /
					<a href="https://gitlab.com/vcla/vrgym">Code</a> <br>
					<pkeywords> Propose VRGym, a virtual reality testbed for realistic human-robot interaction. VRGym emphasizes on building and training both physical and interactive agents for robotics, machine learning, and cognitive science. </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

	  <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/icmlw19vrkitchen.gif" alt="ICMLW 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>VRKitchen: an Interactive 3D Virtual Environment for Task-oriented Learning</papertitle> <br>
					<a href="https://xfgao.github.io/" ,="" style="color : #696969;">Xiaofeng Gao</a>,
					<a href="https://nikepupu.github.io/" ,="" style="color : #696969;">Ran Gong</a>,
					<a href="https://www.tshu.io/" ,="" style="color : #696969;">Tianming Shu</a>,
					<strong>Xu Xie</strong>,
					<a href="https://shuwang0712.github.io/" ,="" style="color : #696969;">Shu Wang</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					International Conference on Machine Learning Workshop (ICMLW) 2019 <br>
 					<a href="https://xfgao.github.io/paper/VRKitchen.pdf">Paper</a> /
					<a href="https://sites.google.com/view/vr-kitchen/">Project</a> /
					<pkeywords> Design and implement a virtual reality (VR) system, VRKitchen, with integrated functions to provide demonstrations for training agents as well as standardized evaluation benchmarks. </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/vrglove19icra.gif" alt="ICRA 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>High-Fidelity Grasping in Virtual Reality using a Glove-based System</papertitle> <br>
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://www.zlz.link/" ,="" style="color : #696969;">Zhenliang Zhang</a>,
					<strong>Xu Xie</strong>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					Yue Liu,
					Yongtian Wang,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Robotics and Automation (ICRA) 2019 <br>
					<a href="https://yzhu.io/publication/glove2019icra/paper.pdf">Paper</a> /
					<a href="https://yzhu.io/publication/glove2019icra/poster.pdf">Poster</a> /
					<a href="https://vimeo.com/317890470">Demo</a> /
					<a href="https://github.com/zzlyw/ICRA19_VRGloveSystem">Code</a> <br>
					<pkeywords> Propose a design that jointly provides hand pose sensing, hand localization, and haptic feedback to facilitate real-time stable grasps in Virtual Reality (VR). Such a glove-based system can simplify the data collection of human manipulations with VR.</pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

	  <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/anomaly19aaaiw.png" alt="AAAIW 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Intention-based Behavioral Anomaly Detection</papertitle> <br>
					<a href="https://fhung65.github.io/" ,="" style="color : #696969;">Fan Hung</a>,
					<strong>Xu Xie</strong>,
					Andrew Fuchs,
					Michael Walton,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969" ;="">Yixin Zhu</a>,
					Doug Lange,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					Association for the Advancement of Artificial Intelligence Workshop (AAAIW) 2019 <br>
					<a href="./resources/aaaiw19hung.pdf">Paper</a> <br>
					<pkeywords> Propose a method for detection of anomalous behaviors based on agent intent formulated using agent-based Lagrangian Mechanics. The method simultaneously learns to predict intended agent goals and detect anomalies based on predictions.</pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>


      <table width="110%" border="0" align="center" cellpadding="5">
        <tbody><tr>
				<td width="20%" valign="top">
					<br>
					<img src="./resources/gloveaction18icra.gif" alt="ICRA 2018" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle>Unsupervised Learning using Hierarchical Models for Hand-Object Interactions</papertitle> <br>
          	<div class="authors">
          			<strong>Xu Xie</strong>
          			<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://www.mjedmonds.com/" ,="" style="color : #696969;">Mark Edmonds</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969" ;="">Yixin Zhu</a>,
					<a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock/" ,="" style="color : #696969;">Brandon Rothrock</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a>
			</div><br>
 				  IEEE International Conference on Robotics and Automation (ICRA) 2018 <br>
					<a href="https://yzhu.io/publication/glove2018icra/paper.pdf">Paper</a> /
					<a href="https://yzhu.io/publication/glove2018icra/poster.pdf">Poster</a> /
					<a href="https://vimeo.com/259416784">Demo</a> /
					<a href="https://github.com/xuxie1031/UnsupervisedGloveAction">Code</a> /
					<a href="./resources/xie2018unsupervised.bib">Bibtex</a> /
					<a href="https://yzhu.io/publication/glove2018icra/">Project</a> <br>
					<pkeywords> Propose an unsupervised learning approach for manipulation event segmentation and manipulation event parsing. The proposed framework incorporates hand pose kinematics and contact forces using a low-cost easy-to-replicate tactile glove. </pkeywords> <br>
					<br><br>
				</td>
	    </tr></tbody></table>

     <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="top">
					<img src="./resources/arpatching2018icra.gif" alt="ICRA 2018" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Interactive Robot Knowledge Patching using Augmented Reality</papertitle> <br>
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					Yaofang Zhang,
					Wenwen Si,
					<strong>Xu Xie</strong>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a> ,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Robotics and Automation (ICRA) 2018 <br>
					<a href="https://yzhu.io/publication/ar2018icra/paper.pdf">Paper</a> /
					<a href="https://yzhu.io/publication/ar2018icra/poster.pdf">Poster</a> / 
					<a href="https://vimeo.com/256261673">Demo</a> /
					<a href="https://github.com/xiaozhuchacha/AOG_AR">Code</a> /
					<a href="./resources/liu2018interactive.bib">Bibtex</a> /
					<a href="https://liuhx111.github.io/AR.html">Project</a> <br>
					<pkeywords>Present a novel Augmented Reality (AR) approach, through Microsoft HoloLens, to address the challenging problems of diagnosing, teaching, and patching interpretable knowledge of a robot.</pkeywords> <br>
					<br> <br>
				</td>
			</tr>
			</tbody></table>
    
	    
      <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="top">
					<img src="./resources/openbottle17iros.gif" alt="IROS 2017" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle>Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles</papertitle> <br>
					<a href="http://www.mjedmonds.com/" ,="" style="color : #696969;">Mark Edmonds</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<strong>Xu Xie</strong>,
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock/" ,="" style="color : #696969;">Brandon Rothrock</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Intelligent Robots and Systems (IROS) 2017 <br>
					<a href="https://yzhu.io/publication/openbottle2017iros/paper.pdf">Paper</a> / 
					<a href="https://vimeo.com/227504384">Demo</a> /
					<a href="https://github.com/xiaozhuchacha/OpenBottle">Code</a> /
					<a href="./resources/edmonds2017feeling.bib">Bibtex</a> /
					<a href="https://fen9.github.io/projects/open_bottles/index.html">Project</a> <br>
					<pkeywords> Learn a manipulation model to execute tasks with multiple stages and variable structure, which typically are not suitable for most robot manipulation approaches. The model is learned from human demonstration using a tactile glove that measures both hand pose and contact forces.</pkeywords> <br>
					<br> <br>
				</td>
			</tr>
			</tbody></table>

      <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/glove17iros.gif" alt="IROS 2017" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle>  A Glove-based System for Studying Hand-Object Manipulation via Joint Pose and Force Sensing </papertitle> <br>
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<strong>Xu Xie</strong>,
					Matt Millar,
					<a href="http://www.mjedmonds.com/" ,="" style="color : #696969;">Mark Edmonds</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="https://samueli.ucla.edu/people/veronica-santos/" ,="" style="color : #696969;">Veronica Santos</a>,
					<a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock/" ,="" style="color : #696969;">Brandon Rothrock</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Intelligent Robots and Systems (IROS) 2017 <br>
 					<a href="https://yzhu.io/publication/glove2017iros/paper.pdf">Paper</a> /
					<a href="https://github.com/xiaozhuchacha/VCLATactileGlove">Code</a> /
					<a href="./resources/liu2017glove.bib">Bibtex</a> /
					<a href="https://liuhx111.github.io/glove.html">Project</a> <br>
 					<pkeywords> Present a design of an easy-to-replicate glove-based system that can reliably perform simultaneous hand pose and force sensing in real time, for the purpose of collecting human hand data during fine manipulative actions. </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
      </tbody></table>

      <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/icip2015.png" alt="icip 2015" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> Rank-Aware Graph Fusion with Contextual Dissimilarity Measurement for Image Retrieval </papertitle> <br>
					<strong>Xu Xie</strong>,
					<a href="http://staff.ustc.edu.cn/~zhwg/" ,="" style="color : #696969;">Wengang Zhou</a>,
					<a href="staff.ustc.edu.cn/~lihq/English.html" ,="" style="color : #696969;">Houqiang Li</a>,
					<a href="http://www.cs.utsa.edu/~qitian/" ,="" style="color : #696969;">Qi Tian</a>
					<br>
 					IEEE International Conference on Image Processing (ICIP) 2015 <br>
 					<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351573">Paper</a> /
 					<a href="./resources/xie2015rank.bib">Bibtex</a> <br>
 					<pkeywords> Propose a rank-aware graph fusion scheme to fuse the results from multiple retrieval methods. Evaluation on two public datasets demonstrates the effectiveness of our approach. </pkeywords>
					<br> <br>
				</td>
			</tr>
      </tbody></table>
       <!--
       <table width="100%" border="0" align="center" cellpadding="10">
      <tr>
				<td width="20%" valign="center">
					<img src="thumbnails/vcip15.png" alt="vcip 2015" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> Image Set Querying Based Localization</a> </papertitle> <br />
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Lei Deng</a> *,
					<strong>Siyuan Huang</strong> *,
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Yueqi Duan</a>,
					Baohua Chen,
					<a href="http://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html", style="color : #696969;">Jie Zhou</a>,
					<br />
					* Equal contributions 
					<br />
 					IEEE Visual Communications and Image Processing (VCIP) 2015 <br />
 					<a href="http://ieeexplore.ieee.org/document/7457924/">Paper</a> / 
 					<a href="files/vcip2015.bib">Bibtex</a> <br />
 					 					<pkeywords> Propose an approach to deal with the situation when a single image fail to localize in the image-set querying based system.  </pkeywords>
					<br /> <br />
				</td>
			</tr>
      </table>
		-->
		</td>
	</tr>	
	
	</tbody></table>




</div></body></html>