<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0024)https://siyuanhuang.com/ -->
<html class="gr__siyuanhuang_com"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script type="text/javascript" async="" src="./resources/analytics.js.download"></script><script async="" src="./resources/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121802070-1');
</script>
<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
<style type="text/css">
a {
color: #000080;
text-decoration:none;
}

a:focus, a:hover {
color: #000080;
text-decoration:underline;
}

a.hover
{
    text-decoration: none;
}

 a.hover:hover
 {
    text-decoration: underline;
 }
body,td,th {
	font-family: Lora;
	font-size: 16px;
}
papertitle {
	font-family: Lora;
	font-size: 16px;
}
conference {
	font-family: Lora;
	font-size: 16px;
	color: #696969;
}
strong {
	font-family: Lora;
	font-size: 16px;
	font-weight: 600;
}
heading {
	font-family: Lora;
	color: #970024; /*#EB6400;*/
	font-size: 28px;
	font-weight: bold;
}
pvenue {
	display: inline-block;
	color: #500;
}
pkeywords{
	color: #EB6400;
}
</style>
    <!-- Custom Fonts -->
    <link href="./resources/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="./resources/css" rel="stylesheet" type="text/css">
    <link href="./resources/css(1)" rel="stylesheet" type="text/css">
<link rel="icon" type="image/png" href="https://siyuanhuang.com/seal_icon.png">
<link href="./resources/styles.css" rel="stylesheet">
<title>Xu Xie - VCLA, UCLA</title>

<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<link href="./resources/css(2)" rel="stylesheet" type="text/css">
</head>
<body data-gr-c-s-loaded="true">
<div id="Banner">
    <div height="80" id="header" style="background-color:#000080; color: #000080">
      <center>
        <table width="1050" height="80" border="0">
          <tbody><tr>
		<td halign="center">
			<p align="center"><font size="6"><font color="#FFFFFF">Xu Xie</font> </font></p><font size="6">
		</font></td>
	</tr>
        </tbody></table>
      </center>
    </div>
	<table width="1000" border="0" align="center" cellpadding="10">

	<tbody><tr>
		<td>
			<table width="100%" border="0" align="center" cellpadding="10">
			<tbody><tr>
							
				<td width="15%" valign="top">
					<img src="./resources/self.png" alt="Seattle" width="100%">
				</td>
				<td width="50%" valign="top">
			I am a third year Ph.D. student in the Department of Statistics at University of California, Los Angeles. I am advised by <a href="http://www.stat.ucla.edu/~sczhu/">Professor Song-Chun Zhu</a> at the <a href="https://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>. Before coming to UCLA, I graduated from <a href="https://www.ustc.edu.cn/">University of Science and Technology of China</a> with a Bachelors in <a href="https://en.sist.ustc.edu.cn/">School of Information Science and Technology</a>.<p></p>
			<br>My research interests lies on robotics, virtual reality, (inverse) reinforcement learning. I currently focus on the problem of modeling agent intention in the driving scenario which involves agent mind, task plan and value estimation. My research highlights achieving machine intelligence through simulations. I am leveraging simulation to promote robot learning on complicated tasks. 
         
          <center>
          <a href="mailto:xuxie@ucla.edu">E-Mail</a> / 
          <a href="./resources/XieXu-CV.pdf">CV</a> 
          <!-- <a href="https://scholar.google.com/citations?user=1NN7Ee8AAAAJ&amp;hl=en&amp;citsig=AMstHGQ_eDX956QVraGalRBAEWGG0ltzRw"> Google Scholar</a> -->
          </center>

			</td></tr>
			</tbody></table>
		</td>
	</tr>

	<tr>
		<td>
      <heading>News</heading>
      <ul>
		<li> <font color="red"><strong>NEW</strong></font>&nbsp;&nbsp; 06/2019 One Paper accepted by IROS 2019.</li>
		<li> Best Paper Award in <a href="https://www.acmturc.com/2019">ACM Turing Celebration Conference (ACM TURC) 2019 </a>.</li>
      	<li> 03/2019 One Paper accepted by ACM TURC 2019.
      	</li><li> 01/2019 One Paper accepted by ICRA 2019.
      	</li><li> 01/2018 Two Papers accepted by ICRA 2018. </li>
		<li> 06/2018 Two Papers accepted by IROS 2017. </li>
	</ul> <br><br><br>
     
   
      <heading>Publications</heading>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/iros19.png" alt="IROS 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Learning Vritual Grasp with Failed Demonstrations via Bayesian Inverse Reinforcement Learning</papertitle> <br>
					<strong>Xu Xie</strong>,
					<a href="https://changyangli.github.io/" ,="" style="color : #696969;">Changyang Li</a>,
					<a href="http://wellyzhang.github.io/" ,="" style="color : #696969;">Chi Zhang</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Intelligent Robots and Systems (IROS) 2019 <br>
 					<a href="./resources/iros19xie.pdf">Paper</a> /
					<a href="https://vimeo.com/350872475">Demo</a> /
					<a href="https://xuxie1031.github.io/projects/VRGrasp/VRGraspProj.html">Project</a> /
					<a href="https://github.com/xuxie1031/VRGraspIRLEnv">Code</a> <br>
					<pkeywords> Propose Bayesian Inverse Reinforcement Learning with Failure (BIRLF), which makes use of failed demonstrations that were often ignored or filtered in previous methods due to the difficulties to incorporate them in addition to the successful ones. </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/acmturc19.gif" alt="ACMTURC 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>VRGym: A Virtual Testbed for Physical and Interactive AI</papertitle> <br>
					<strong>Xu Xie</strong>,
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://www.zlz.link/" ,="" style="color : #696969;">Zhenliang Zhang</a>,
					<a href="https://yuxinqiu.github.io/" ,="" style="color : #696969;">Yuxing Qiu</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					ACM Turing Celebration Conference (ACM TURC) 2019 <br>
 					<a href="./resources/sigai19xie.pdf">Paper</a> /
					<a href="./resources/sigai19xie_sppl.pdf">Supplementary</a> /
					<a href="https://vimeo.com/327629749">Demo</a> /
					<a href="https://xuxie1031.github.io/projects/VRGym/VRGymProj.html">Project</a> <br>
					<pkeywords> Propose VRGym, a virtual reality testbed for realistic human-robot interaction. VRGym emphasizes on building and training both physical and interactive agents for robotics, machine learning, and cognitive science. </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/vrglove19icra.gif" alt="ICRA 2019" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>High-Fidelity Grasping in Virtual Reality using a Glove-based System</papertitle> <br>
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://www.zlz.link/" ,="" style="color : #696969;">Zhenliang Zhang</a>,
					<strong>Xu Xie</strong>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					Yue Liu,
					Yongtian Wang,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Robotics and Automation (ICRA) 2019 <br>
					<a href="https://www.yzhu.io/projects/icra19_vrglove/vrglove2019icra.pdf">Paper</a> /
					<a href="https://www.yzhu.io/projects/icra19_vrglove/vrglove2019icra_poster.pdf">Poster</a> /
					<a href="https://vimeo.com/317890470">Demo</a> /
					<a href="https://github.com/zzlyw/ICRA19_VRGloveSystem">Code</a> <br>
					<pkeywords> Propose a design that jointly provides hand pose sensing, hand localization, and haptic feedback to facilitate real-time stable grasps in Virtual Reality (VR). Such a glove-based system can simplify the data collection of human manipulations with VR.</pkeywords> <br>
					<br> <br>
				</td>
			</tr>
		</tbody></table>

      <table width="110%" border="0" align="center" cellpadding="5">
        <tbody><tr>
				<td width="20%" valign="top">
					<br>
					<img src="./resources/gloveaction18icra.gif" alt="ICRA 2018" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle>Unsupervised Learning using Hierarchical Models for Hand-Object Interactions</papertitle> <br>
          	<div class="authors">
          			<strong>Xu Xie</strong>
          			<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://www.mjedmonds.com/" ,="" style="color : #696969;">Mark Edmonds</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969" ;="">Yixin Zhu</a>,
					<a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock/" ,="" style="color : #696969;">Brandon Rothrock</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a>
			</div><br>
 				  IEEE International Conference on Robotics and Automation (ICRA) 2018 <br>
					<a href="https://www.yzhu.io/projects/icra18_gloveaction/gloveaction2018icra.pdf">Paper</a> /
					<a href="https://www.yzhu.io/projects/icra18_gloveaction/gloveaction2018icra_poster.pdf">Poster</a> /
					<a href="https://vimeo.com/259416784">Demo</a> /
					<a href="https://github.com/xuxie1031/UnsupervisedGloveAction">Code</a> /
					<a href="./resources/xie2018unsupervised.bib">Bibtex</a> /
					<a href="https://www.yzhu.io/projects/icra18_gloveaction/index.html">Project</a> <br>
					<pkeywords> Propose an unsupervised learning approach for manipulation event segmentation and manipulation event parsing. The proposed framework incorporates hand pose kinematics and contact forces using a low-cost easy-to-replicate tactile glove. </pkeywords> <br>
					<br><br>
				</td>
	    </tr></tbody></table>

     <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="top">
					<img src="./resources/arpatching2018icra.gif" alt="ICRA 2018" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Interactive Robot Knowledge Patching using Augmented Reality</papertitle> <br>
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					Yaofang Zhang,
					Wenwen Si,
					<strong>Xu Xie</strong>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a> ,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Robotics and Automation (ICRA) 2018 <br>
					<a href="https://www.yzhu.io/projects/icra18_arpatching/arpatching2018icra.pdf">Paper</a> /
					<a href="https://www.yzhu.io/projects/icra18_arpatching/arpatching2018icra_poster.pdf">Poster</a> / 
					<a href="https://vimeo.com/256261673">Demo</a> /
					<a href="https://github.com/xiaozhuchacha/AOG_AR">Code</a> /
					<a href="./resources/liu2018interactive.bib">Bibtex</a> /
					<a href="https://www.yzhu.io/projects/icra18_arpatching/index.html">Project</a> <br>
					<pkeywords>Present a novel Augmented Reality (AR) approach, through Microsoft HoloLens, to address the challenging problems of diagnosing, teaching, and patching interpretable knowledge of a robot.</pkeywords> <br>
					<br> <br>
				</td>
			</tr>
			</tbody></table>
    
	    
      <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="top">
					<img src="./resources/openbottle17iros.gif" alt="IROS 2017" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle>Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles</papertitle> <br>
					<a href="http://www.mjedmonds.com/" ,="" style="color : #696969;">Mark Edmonds</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<strong>Xu Xie</strong>,
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<a href="http://web.cs.ucla.edu/~syqi/" ,="" style="color : #696969;">Siyuan Qi</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock/" ,="" style="color : #696969;">Brandon Rothrock</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Intelligent Robots and Systems (IROS) 2017 <br>
					<a href="https://www.yzhu.io/projects/iros17_openbottle/openbottle2017iros.pdf">Paper</a> / 
					<a href="https://vimeo.com/227504384">Demo</a> /
					<a href="https://github.com/xiaozhuchacha/OpenBottle">Code</a> /
					<a href="./resources/edmonds2017feeling.bib">Bibtex</a> /
					<a href="https://www.yzhu.io/projects/iros17_openbottle/index.html">Project</a> <br>
					<pkeywords> Learn a manipulation model to execute tasks with multiple stages and variable structure, which typically are not suitable for most robot manipulation approaches. The model is learned from human demonstration using a tactile glove that measures both hand pose and contact forces.</pkeywords> <br>
					<br> <br>
				</td>
			</tr>
			</tbody></table>

      <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/glove17iros.gif" alt="IROS 2017" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle>  A Glove-based System for Studying Hand-Object Manipulation via Joint Pose and Force Sensing </papertitle> <br>
					<a href="https://liuhx111.github.io/" ,="" style="color : #696969;">Hangxin Liu</a>,
					<strong>Xu Xie</strong>,
					Matt Millar,
					<a href="http://www.mjedmonds.com/" ,="" style="color : #696969;">Mark Edmonds</a>,
					<a href="https://fen9.github.io/" ,="" style="color : #696969;">Feng Gao</a>,
					<a href="https://www.yzhu.io/" ,="" style="color : #696969;">Yixin Zhu</a>,
					<a href="https://samueli.ucla.edu/people/veronica-santos/" ,="" style="color : #696969;">Veronica Santos</a>,
					<a href="https://www-robotics.jpl.nasa.gov/people/Brandon_Rothrock/" ,="" style="color : #696969;">Brandon Rothrock</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/" ,="" style="color : #696969;">Song-Chun Zhu</a> <br>
 					IEEE International Conference on Intelligent Robots and Systems (IROS) 2017 <br>
 					<a href="https://www.yzhu.io/projects/iros17_glove/glove2017iros.pdf">Paper</a> /
					<a href="https://github.com/xiaozhuchacha/VCLATactileGlove">Code</a> /
					<a href="./resources/liu2017glove.bib">Bibtex</a> /
					<a href="https://www.yzhu.io/projects/iros17_glove/index.html">Project</a> <br>
 					<pkeywords> Present a design of an easy-to-replicate glove-based system that can reliably perform simultaneous hand pose and force sensing in real time, for the purpose of collecting human hand data during fine manipulative actions. </pkeywords> <br>
					<br> <br>
				</td>
			</tr>
      </tbody></table>

      <table width="110%" border="0" align="center" cellpadding="5">
      <tbody><tr>
				<td width="20%" valign="center">
					<img src="./resources/icip2015.png" alt="icip 2015" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> Rank-Aware Graph Fusion with Contextual Dissimilarity Measurement for Image Retrieval </papertitle> <br>
					<strong>Xu Xie</strong>,
					<a href="http://staff.ustc.edu.cn/~zhwg/" ,="" style="color : #696969;">Wengang Zhou</a>,
					<a href="staff.ustc.edu.cn/~lihq/English.html" ,="" style="color : #696969;">Houqiang Li</a>,
					<a href="http://www.cs.utsa.edu/~qitian/" ,="" style="color : #696969;">Qi Tian</a>
					<br>
 					IEEE International Conference on Image Processing (ICIP) 2015 <br>
 					<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351573">Paper</a> /
 					<a href="./resources/xie2015rank.bib">Bibtex</a> <br>
 					<pkeywords> Propose a rank-aware graph fusion scheme to fuse the results from multiple retrieval methods. Evaluation on two public datasets demonstrates the effectiveness of our approach. </pkeywords>
					<br> <br>
				</td>
			</tr>
      </tbody></table>
       <!--
       <table width="100%" border="0" align="center" cellpadding="10">
      <tr>
				<td width="20%" valign="center">
					<img src="thumbnails/vcip15.png" alt="vcip 2015" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> Image Set Querying Based Localization</a> </papertitle> <br />
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Lei Deng</a> *,
					<strong>Siyuan Huang</strong> *,
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Yueqi Duan</a>,
					Baohua Chen,
					<a href="http://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html", style="color : #696969;">Jie Zhou</a>,
					<br />
					* Equal contributions 
					<br />
 					IEEE Visual Communications and Image Processing (VCIP) 2015 <br />
 					<a href="http://ieeexplore.ieee.org/document/7457924/">Paper</a> / 
 					<a href="files/vcip2015.bib">Bibtex</a> <br />
 					 					<pkeywords> Propose an approach to deal with the situation when a single image fail to localize in the image-set querying based system.  </pkeywords>
					<br /> <br />
				</td>
			</tr>
      </table>
		-->
		</td>
	</tr>	
	
	</tbody></table>




</div></body></html>